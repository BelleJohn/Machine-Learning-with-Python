{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwY5h-dWycG"
   },
   "source": [
    "## Main Course\n",
    "\n",
    "  Get the code for this tutorial: https://github.com/nicknochnack/ReinforcementLearningCourse.git\n",
    "\n",
    "  Watch the video: https://www.youtube.com/watch?v=Mut_u40Sqz4&list=WL&index=8&t=141s\n",
    "\n",
    "  Note: The code isn't exact the same as the video or repo because the code needs to update and debug. And if you run the code in Colab, it requires extra actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxLj9cLPZTkA"
   },
   "source": [
    "### 1. Import dependencies\n",
    "  Documentation: https://stable-baselines3.readthedocs.io/en/master/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "R4psK4jQY3V5",
    "outputId": "daf2706e-66db-4f21-ac18-9e54afca3fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in c:\\conda\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\conda\\lib\\site-packages (from stable-baselines3) (1.21.5)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\conda\\lib\\site-packages (from stable-baselines3) (2.6.0)\n",
      "Requirement already satisfied: pandas in c:\\conda\\lib\\site-packages (from stable-baselines3) (1.4.2)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in c:\\conda\\lib\\site-packages (from stable-baselines3) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\conda\\lib\\site-packages (from stable-baselines3) (3.5.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\conda\\lib\\site-packages (from stable-baselines3) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\conda\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\conda\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\conda\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\conda\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium<1.1.0,>=0.29.1->stable-baselines3) (3.7.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\conda\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\conda\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\conda\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\conda\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2022.2.0)\n",
      "Requirement already satisfied: filelock in c:\\conda\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\conda\\lib\\site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\conda\\lib\\site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\conda\\lib\\site-packages (from matplotlib->stable-baselines3) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\conda\\lib\\site-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\conda\\lib\\site-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\conda\\lib\\site-packages (from matplotlib->stable-baselines3) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\conda\\lib\\site-packages (from matplotlib->stable-baselines3) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\conda\\lib\\site-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\conda\\lib\\site-packages (from matplotlib->stable-baselines3) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\conda\\lib\\site-packages (from pandas->stable-baselines3) (2021.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CuQDOdiHZYRJ"
   },
   "outputs": [],
   "source": [
    "import os # to search the path\n",
    "#import gym # for openAI gym --> old version\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO # one of algorithms (see the document above)\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # train models in multiple environment at same time; speed up training\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # test model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YngLQHYUbuFb"
   },
   "source": [
    "### 2. Load Environment (Cartpole as example here)\n",
    "\n",
    "The main environment functions are:\n",
    "1. env.reset( ) - reset the environment and obtain initial oberservations\n",
    "2. env.render( ) - visualise the environment\n",
    "3. env.step( ) - apply an action to the environment\n",
    "4. env.close( ) - close down the render frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sfWxoFdRbyBq"
   },
   "outputs": [],
   "source": [
    "# create environment\n",
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:24.0\n",
      "Episode:2 Score:10.0\n",
      "Episode:3 Score:25.0\n",
      "Episode:4 Score:17.0\n",
      "Episode:5 Score:21.0\n"
     ]
    }
   ],
   "source": [
    "# solution: \n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset() # Observations for the environment not just for the pole.\n",
    "    # we will deliver these observations to reinforcement learning agent to learn the optimal value.\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "\n",
    "    while not (done or truncated):\n",
    "        env.render()\n",
    "        #action = random.choice([0,1])\n",
    "        action = env.action_space.sample() # random action\n",
    "        n_state, reward, done, truncated, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "gMzuHeFVV2KR",
    "outputId": "f7e53ed5-5fd0-4e35-b25c-78287dad979f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# env.render(mode='rgb_array')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;66;03m# random action\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     n_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# pass random action\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     score\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Score:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode, score))\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# test environment: episodes is like one full game within the environment. Same environments\n",
    "# have a fixed episode length e.g. cartpole wich is 200 frames. Others are continuous,\n",
    "# e.g. Breakout, play until you run out of lives.\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset() # Observations for the environment not just for the pole.\n",
    "    # we will deliver these observations to reinforcement learning agent to learn the optimal value.\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render() # it doesn't work with colab\n",
    "        # env.render(mode='rgb_array')\n",
    "        action = env.action_space.sample() # random action\n",
    "        n_state, reward, done, info = env.step(action) # pass random action\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UKnvOKeOfQJ",
    "outputId": "cad724b5-2b7f-486c-a336-09161f2f6989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # the action you can take in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4ZKb3uQb5Te"
   },
   "source": [
    "Understanding The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKDMzZUJb6KV",
    "outputId": "d3e528c2-6386-4cb6-8ba6-e33d3b544bec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0-push cart to left, 1-push cart to the right\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cGUJxKMWNGm",
    "outputId": "1d64c8b1-f023-43bd-955e-de0781b20a88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.629841  ,  0.42401126, -0.30284554, -0.8044943 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqdHSxDhOzor",
    "outputId": "8f138c63-ab97-4126-bc0b-252352e589b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (4,) is from env.reset\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2z0vHbAb74k"
   },
   "source": [
    "### 3. Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "k8OF7dpPcA2s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wPUJmPwTWQYA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2549 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1704        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008285886 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00544    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 57.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1599        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011698946 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0799      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 36.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1543        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010194197 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 52.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1520         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060371226 |\n",
      "|    clip_fraction        | 0.0669       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.607       |\n",
      "|    explained_variance   | 0.239        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.6         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.016       |\n",
      "|    value_loss           | 64.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1505         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069621666 |\n",
      "|    clip_fraction        | 0.0549       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | 0.546        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    value_loss           | 51.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1497         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077944132 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.593       |\n",
      "|    explained_variance   | 0.543        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.7         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00908     |\n",
      "|    value_loss           | 52.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1477        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005258903 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1463         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087405685 |\n",
      "|    clip_fraction        | 0.0872       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.644        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17           |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 54.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1456        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007891272 |\n",
      "|    clip_fraction        | 0.0743      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.51        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00688    |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1b7f5f0a3d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5fuPCBpcB9J"
   },
   "source": [
    "### 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zbeSVFgucGbV"
   },
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\Saved Models\\PPO_model\n"
     ]
    }
   ],
   "source": [
    "print(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nPMNRZMUWTkr"
   },
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GL-v7kluWWAa"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DQYlDcBdWXlM"
   },
   "outputs": [],
   "source": [
    "load_model = PPO.load('Training\\Saved Models\\PPO_model', env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy4Dy_xdcGyT"
   },
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "0uiWVdp4cNFg"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CIGpq7McWfL-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "C:\\conda\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:244: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ofA-REbOWilr"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-eHMVwMcP_c"
   },
   "source": [
    "### 6. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "quGxfq9-cSGO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info [{'TimeLimit.truncated': True, 'terminal_observation': array([-0.2466684 , -0.05472602,  0.04111669,  0.3380593 ], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print('info', info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "HQkAxkC5Wmr-"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwbEozNHcSmH"
   },
   "source": [
    "### 7. Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RwAuoGdicV-P"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_log_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mlog_path\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPO_3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_path' is not defined"
     ]
    }
   ],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "x0RoqC08WqqY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-JrrHDzcX5t"
   },
   "source": [
    "\n",
    "### 8. Adding a callback to the training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsnxtixYcZWr"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST3pECKAWurL"
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VtRpD4OXZxq"
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ti_4F_OmXaJ4"
   },
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
    "eval_callback = EvalCallback(env,\n",
    "                             callback_on_new_best=stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=save_path,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDjJsF0FXbrS"
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXDt8XdgXdiU"
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqWfJcvhXfAF"
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved Models', 'best_model')\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-hP81huXgZ8"
   },
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzM6YYHeXiAz"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZxZOITTcZ2f"
   },
   "source": [
    "### 9. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKSIOfeOcd3W"
   },
   "outputs": [],
   "source": [
    "net_arch=[dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbXroYp0Xm-S"
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, policy_kwargs={'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFJbm7zCXngK"
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50GUqDyRcfYa"
   },
   "source": [
    "### 10. Using an Alternate Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWGqJd04chXu"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UG1lSwfeXqxP"
   },
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vI2Yzk1eXsjE"
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkAw8XI8XubY"
   },
   "outputs": [],
   "source": [
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTkDPremXy4z"
   },
   "outputs": [],
   "source": [
    "model.save(dqn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mrV8f7uX0UO"
   },
   "outputs": [],
   "source": [
    "model = DQN.load(dqn_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwT496mAX17a"
   },
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RAw-XPZX3nc"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mushgcz2W7fQ"
   },
   "source": [
    "## Project 1 - Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74rt617OZgzI"
   },
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zvscFXpXAki"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTuZgEUzZkAX"
   },
   "source": [
    "### 2. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4RmxLuvZ0bK"
   },
   "outputs": [],
   "source": [
    "environment_name = \"Breakout-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4iaLoNDZ3wf"
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clzGhtSLZ6Eb"
   },
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Y3L4pbPZ8BL"
   },
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHadBz57Z9ft"
   },
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Shmozt5vZp7y"
   },
   "source": [
    "### 3. Vectorise Environment and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2I5WzF4aB1l"
   },
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=4, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWvXCzI-aD30"
   },
   "outputs": [],
   "source": [
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mycpVruIaGHT"
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOFUOhE8aISY"
   },
   "outputs": [],
   "source": [
    "model = A2C(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oKRvqpUaKbp"
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6OYUSZ5ZvGS"
   },
   "source": [
    "### 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7gQLdwqaQph"
   },
   "outputs": [],
   "source": [
    "a2c_path = os.path.join('Training', 'Saved Models', 'A2C_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzSFsmneaTtK"
   },
   "outputs": [],
   "source": [
    "model.save(a2c_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FD4tTUSaWPp"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu3LvmGfaXyI"
   },
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4Z3CNHiaZWn"
   },
   "outputs": [],
   "source": [
    "model = A2C.load(a2c_path, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuwOMQVXZw8y"
   },
   "source": [
    "### 5. Evaluate and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui9uU29Qab5I"
   },
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa3H3z-Sac94"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBwwKe6DagDl"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQxOev09XBOf"
   },
   "source": [
    "## Project 2 - Self Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKihBS5MapVE"
   },
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVAtzMD7XF2a"
   },
   "outputs": [],
   "source": [
    "#Install SWIG https://sourceforge.net/projects/swig/files/swigwin/swigwin-4.0.2/swigwin-4.0.2.zip/download?use_mirror=ixpeering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQwMRC6hatID"
   },
   "outputs": [],
   "source": [
    "!pip install gym[box2d] pyglet==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwkG6OQ8avqk"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUmiDkY-awpW"
   },
   "source": [
    "### 2. Test Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9ftsbqUbJWb"
   },
   "outputs": [],
   "source": [
    "environment_name = \"CarRacing-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GiuGiXLbKfW"
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JT27x533bNNC"
   },
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6KdN_GYbQ-T"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bloj6QEvbQ72"
   },
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiabKz1bbUNm"
   },
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb-7NnbVa6mo"
   },
   "source": [
    "### 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufj5DcMEbWze"
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qE9thixwbZHW"
   },
   "outputs": [],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7bU6guRbbmo"
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulqCX8fYa8kJ"
   },
   "source": [
    "### 4. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJlvfDLzbgQN"
   },
   "outputs": [],
   "source": [
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Driving_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jvRViD-bgOa"
   },
   "outputs": [],
   "source": [
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wf_rezWebBdt"
   },
   "source": [
    "### 5. Evaluate and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwa-BzN3bioz"
   },
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gioHQU_bkDa"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rsHp9t7bk8I"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONo9EU4wbk4J"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLnlgSQRXGnC"
   },
   "source": [
    "## Project 3 - Custom Environment\n",
    "\n",
    "https://sourceforge.net/projects/swig/files/swigwin/swigwin-4.0.2/swigwin-4.0.2.zip/download?use_mirror=ixpeering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9UN4Xn9b2Hu"
   },
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxRZKTs7XPmP"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5ogwOXab5rF"
   },
   "source": [
    "### 2. Types of Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nopuExjb7-H"
   },
   "outputs": [],
   "source": [
    "Discrete(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypyCfTupdHX5"
   },
   "outputs": [],
   "source": [
    "Box(0,1,shape=(3,3)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liaScd_SdI9_"
   },
   "outputs": [],
   "source": [
    "Box(0,255,shape=(3,3), dtype=int).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99pyXfxpdJgf"
   },
   "outputs": [],
   "source": [
    "Tuple((Discrete(2), Box(0,100, shape=(1,)))).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXn3n4LOdJeq"
   },
   "outputs": [],
   "source": [
    "Dict({'height':Discrete(2), \"speed\":Box(0,100, shape=(1,))}).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpMjUaAedI8L"
   },
   "outputs": [],
   "source": [
    "MultiBinary(4).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlL2fVMZdNqB"
   },
   "outputs": [],
   "source": [
    "MultiDiscrete([5,2,2]).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RtKWnZgb-CB"
   },
   "source": [
    "### 3. Building an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssku64wYb-0C"
   },
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Temperature array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start temp\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Set shower length\n",
    "        self.shower_length = 60\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 temperature\n",
    "        # 1 -1 = 0\n",
    "        # 2 -1 = 1 temperature\n",
    "        self.state += action -1\n",
    "        # Reduce shower length by 1 second\n",
    "        self.shower_length -= 1\n",
    "\n",
    "        # Calculate reward\n",
    "        if self.state >=37 and self.state <=39:\n",
    "            reward =1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Check if shower is done\n",
    "        if self.shower_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Apply temperature noise\n",
    "        #self.state += random.randint(-1,1)\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = np.array([38 + random.randint(-3,3)]).astype(float)\n",
    "        # Reset shower time\n",
    "        self.shower_length = 60\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7C5fmMnda2P"
   },
   "outputs": [],
   "source": [
    "env=ShowerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9c4Jl_rdcMt"
   },
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qsXSFJuddu4"
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3021JLhudfjC"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1BpYOX2dg7T"
   },
   "outputs": [],
   "source": [
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_NiMfDHcAWu"
   },
   "source": [
    "### 4. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJxXikW1dkSa"
   },
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWk70g0SdmgY"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OcqGCthcC0j"
   },
   "source": [
    "### 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkoKZEkHcD-I"
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qaxjv2X3dqa6"
   },
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mkq4K8Mxdr2l"
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOV3Z87TcGfK"
   },
   "source": [
    "### 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsypknzNcIpP"
   },
   "outputs": [],
   "source": [
    "model.save('PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mynrstd0dtia"
   },
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
