{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Image Processing and Handling WS 2018/19</h1>\n",
    "\n",
    "Exercise instructor: Marko Jovanović, mjovanovic@mi.rwth-aachen.de\n",
    "\n",
    "<strong style=\"color: red\">Notice: </strong>Attendance to <strong>all</strong> exercises sessions <strong>is mandatory</strong>. However, submitted exercise solutions aren't graded nor they present a prerequiste for the exam, but you will receive feedback on your submitted solutions.\n",
    "\n",
    "The exercise sessions are held from 12.30-14.00 on the following dates at COMA 1:\n",
    "\n",
    "22.10.2018 - OpenCV and Python intro (this session)<br />\n",
    "05.11.2018 - Image Enhancement<br />\n",
    "19.11.2018 - Fourier Transform<br />\n",
    "03.12.2018 - Low-level Image Segmentation<br />\n",
    "17.12.2018 - High-level Image Segmentation<br />\n",
    "14.01.2019 - Feature Descriptors and Interest Points<br />\n",
    "<strong>21.01.2019 08.30-10.00 - Image Registration</strong><br />\n",
    "28.01.2019 <strong style=\"color: red\">08.30-10.00</strong> - Solving a Problem<br />\n",
    "\n",
    "The topics are provided here for your orientation and are subject to change, in accordance with the lectures.\n",
    "\n",
    "<h2>Exercise 7: Image Registration</h2>\n",
    "\n",
    "Due date: <strong>28.01.2018</strong>\n",
    "\n",
    "<h3>List of Tasks</h3>\n",
    "<ul>\n",
    "    <li><a href=\"#task1\">Task 1: Manual Registration with OpenCV: Finding Affine Transformation</a></li>\n",
    "    <li><a href=\"#task2\">Task 2: Feature-Based Registration</a></li>\n",
    "    <li><a href=\"#task3\">Task 3: Template matching</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image registration is the process of transforming different sets of data into one coordinate system.\n",
    "Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. In medical\n",
    "imaging, a typical application would be alignment of CT and MRI scans: each of these modalities\n",
    "highlights different characteristics of a patient’s anatomy, and a registration process is required before\n",
    "the doctors can overlay both images. Another example would be scanning of the patient twice at a\n",
    "few months interval, when the two scans shall registered in order to assess the evolution of a disease.\n",
    "Registration is usually performed based on features, which may be specific structures in the image such\n",
    "as points, edges or objects. Features may also be the result of a general neighborhood operation or\n",
    "feature detection applied to the image. Registration methods may be classified based on the level of\n",
    "automation they provide:\n",
    "\n",
    "<ul>\n",
    "    <li><em>Manual methods</em> provide tools to align the images manually. Usually, interactive methods reduce\n",
    "user bias by performing certain key operations automatically while still relying on the user to guide\n",
    "    the registration. Using manual methods, the result is, however, user-dependent.</li>\n",
    "    <li><em>Semi-automatic methods</em> perform bigger number of the registration steps automatically but still\n",
    "depend on the user specifying parameters and landmarks or verifying the correctness of a registra-\n",
    "    tion.</li>\n",
    "    <li><em>Automatic methods</em> do not allow any user interaction and perform all registration steps in a self-driven fashion.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays most often image registration is conducted using manual or semi-automatic approaches. In\n",
    "such cases, GUI tools are used to assist experts with selection of correspondences. Further transformation\n",
    "can be computed automatically using existing toolsets.\n",
    "In order to understand the how the transformation works and which functions shall be applied, it is\n",
    "necessary to differentiate transformation types:\n",
    "\n",
    "<ul><li><strong>Rigid transformation</strong> of a vector space preserves distances between every pair of points. It involves\n",
    "    <em>rotations, translations, reflections</em>, or their combination. Sometimes reflections are excluded from\n",
    "the definition of a rigid transformation by imposing that the transformation also preserve the handedness of figures in the Euclidean space. To avoid ambiguity, this smaller class of transformations is known as proper rigid transformations (informally, also known as roto-translations). In general, any proper rigid transformation can be decomposed as a rotation followed by a translation, while any rigid transformation can be decomposed as an improper rotation followed by a translation (or as a sequence of reflections). Any object will keep the same shape and size after a proper rigid\n",
    "transformation.</li>\n",
    "    <li><strong>Affine transformation</strong> involves in addition to above <em>shear</em> and <em>scaling</em> (all rigid transformations\n",
    "are examples of affine transformations). Affine transformations are generalizations of Euclidean transformations. Under affine transformations, lines transforms to lines; but, circles become ellipses. An affine transformation preserves collinearity between points, parallelism, convexity of sets, ratios of lengths along a line, and barycenters of weighted collections of points. Lengths and angles are not preserved.</li>\n",
    "    <li><strong>Orthographic projection</strong> deals with the conversion of 3D world into 2D image. It is a form of\n",
    "parallel projection, in which all the projection lines are orthogonal to the projection plane, resulting in every plane of the scene appearing in affine transformation on the viewing surface. The obverse of an orthographic projection is an oblique projection, which is a parallel projection in which the projection lines are not orthogonal to the projection plane. The term orthographic is sometimes reserved specifically for depictions of objects where the principal axes or planes of the object are also parallel with the projection plane, but these are better known as multiview projections. Further, when the principal planes or axes of an object in an orthographic projection are not parallel with\n",
    "the proje ction plane, but are rather tilted to reveal multiple sides of the object, the projection is called an axonometric projection. Sub-types of multiview projection include plans, elevations and sections. Sub-types of axonometric projection include isometric, dimetric and trimetric projections. Common names for orthographic projections include plane, cross-section, bird’s-eye, and elevation.\n",
    "</li>\n",
    "<li><strong>Perspective projection</strong> also deals with the conversion of 3D world into 2D image. When the\n",
    "human eye views a scene, objects in the distance appear smaller than objects close by – this is known as perspective. While orthographic projection ignores this effect to allow accurate measurements, perspective projection shows distant objects as smaller to provide additional realism. It does not preserve parallel lines, or lengths, or equispaced points. A perspective projection can be precisely specified through four pairs of corresponding points. For a deeper understanding of perspective projection it is necessary to learn the concept of frame of reference: frame of reference\n",
    "is basically a set of values in relation to which we measure something. In order to analyze a 3D world/image/scene, five different frame of references are required: object (for modeling objects), world (for co-relating objects in a 3D world), camera (used to relate objects with respect of the  camera), image (used to describe how 3D points are mapped in a 2D image plane), pixel (each pixel has a value of pixel coordinates). The perspective transformation is then is a combination of the following steps: object coordinates (3D) → world coordinates (3D) → camera coordinates\n",
    "(3D) → image plane coordinates (2D) → pixel coordinates (2D).</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1\"></a>\n",
    "<h3 style=\"color: red\">Task 1: Manual Registration with OpenCV: Finding Affine Transformation</h3>\n",
    "\n",
    "OpenCV comes with a set of functions designed for different kinds of transformations:\n",
    "<ul><li><code>getAffineTransform()</code> calculates an affine transform from three pairs of the corresponding points.</li>\n",
    "    <li><code>estimateRigidTransform()</code> computes an optimal affine transformation between two 2D point sets.</li>\n",
    "<li><code>getPerspectiveTransform()</code> calculates a perspective transform from four pairs of the corresponding points.</li>\n",
    "    <li><code>findHomography()</code> finds a perspective transformation between two planes.</li>\n",
    "</ul>\n",
    "\n",
    "<strong style=\"color: red\">Programming Task: </strong>Find affine transformation between the images <code>screw-1.tif</code> and <code>screw-2.tif</code>. Note that you will need to find 3 pairs of corresponding points in the two images. Apply the transformation matrix to your source image and verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task2\"></a>\n",
    "<h3 style=\"color: red\">Task 2: Feature-Based Registration</h3>\n",
    "\n",
    "Image registration can be performed automatically using interest points. The general pipeline is as\n",
    "follows:\n",
    "<ol>\n",
    "<li>Find points or areas which are invariant to different kind of transformations (translation, rotation,\n",
    "scale) and describe them using one of the existing point descriptors (SIFT, ORB, SURF, etc.). You have learned about such features in the last exercise.</li>\n",
    "<li>After we have found interesting points in all the images that we want to register we try to find the corresponds between the points by calculating some distance measures (for example Euclidean\n",
    "distance) between these descriptors.</li>\n",
    "<li>After we establish some correspondences between the points in different images, we calculate the transformation between them (the number of corresponding points depends on the type of transformation we want to find: rigid, affine, perspective).</li>\n",
    "<li><a href=\"http://www.cse.yorku.ca/~kosta/CompVis_Notes/ransac.pdf\">Random sample consensus (RANSAC)</a> algorithm or a robust distance measure can be used to find outliers.</li>\n",
    "<li>The found transformation is applied to the image we want to register.</li>\n",
    "</ol>\n",
    "\n",
    "As you have already learned there are many possible options to describe features of interest. Similarly, there are several feature matchers available in OpenCV. One of them is brute force matcher represented in OpenCV by the class <code>BFMatcher</code>. The transformation is found using <code><a href=\"https://docs.opencv.org/3.3.1/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780\">findHomography()</a></code>. The function <code>perspectiveTransform()</code> can be used to apply the transformation.\n",
    "\n",
    "<strong style=\"color: red\">Programming Task:</strong> Using your mobile camera, take two photos: a photo of an object's close-up (for example, a book) and a photo of this object in the scene placed under a different angle and surrounded by other objects. Make sure that your object is feature-rich. Apply a detector of your choice to find points of interest in both photographs. We recommend using the ORB detector for this task, but you can try using any other extractor as well.\n",
    "\n",
    "Then, apply the brute force matcher and visualize your matches.\n",
    "\n",
    "After that, find the homography. Use the found homography to apply the perspective transformation. Can you visualize the found object by drawing a bounding box?\n",
    "\n",
    "<strong>Remark: </strong>Don't forget to upload the photos you have taken with your camera :)\n",
    "\n",
    "<strong>Remark:</strong> Read <a href=\"https://docs.opencv.org/3.4.1/dc/dc3/tutorial_py_matcher.html\">OpenCV's tutorial on Feature Matching.</a> Use <code>drawMatches()</code> to visualize your matching results. <a href=\"https://docs.opencv.org/3.4.1/d4/d5d/group__features2d__draw.html\">(see reference here)</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3\"></a>\n",
    "<h3 style=\"color: red\">Task 3: Template matching</h3>\n",
    "    \n",
    "Template matching is a technique for finding areas of an image that match (are similar) to a template image (patch). OpenCV comes with a function <code>matchTemplate()</code> for this purpose. It slides the template image over the input image and compares the template and patch of input image under the template image. Several comparison methods are implemented in OpenCV. It returns a grayscale image, where each pixel denotes how much does the neighbourhood of that pixel match with template.\n",
    "\n",
    "<strong style=\"color: red\">Programming Task:</strong> Download two images <code>photo-eye-1.bmp</code> and <code>Kernel.png</code>. Apply different template matching methods to find the image <code>Kernel.png</code> in the <code>photo-eye-1.bmp</code> using suitable parameters. Which method performs the best and why?\n",
    "\n",
    "<strong style=\"color: red\">Programming Task:</strong> Take two photos of your hand. Crop one so that it contains the hand only. This will be your template. The second photograph will be the scene: besides the hand, it shall also contain some\n",
    "other objects. Apply a suitable template matching method to find the template photo in the scene.\n",
    "Would this method work for the images you took for the Task 3.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
