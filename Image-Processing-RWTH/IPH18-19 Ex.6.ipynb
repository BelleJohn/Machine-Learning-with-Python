{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Image Processing and Handling WS 2018/19</h1>\n",
    "\n",
    "Exercise instructor: Marko Jovanović, mjovanovic@mi.rwth-aachen.de\n",
    "\n",
    "<strong style=\"color: red\">Notice: </strong>Attendance to <strong>all</strong> exercises sessions <strong>is mandatory</strong>. However, submitted exercise solutions aren't graded nor they present a prerequiste for the exam, but you will receive feedback on your submitted solutions.\n",
    "\n",
    "The exercise sessions are held from 12.30-14.00 on the following dates at COMA 1:\n",
    "\n",
    "22.10.2018 - OpenCV and Python intro (this session)<br />\n",
    "05.11.2018 - Image Enhancement<br />\n",
    "19.11.2018 - Fourier Transform<br />\n",
    "03.12.2018 - Low-level Image Segmentation<br />\n",
    "17.12.2018 - High-level Image Segmentation<br />\n",
    "<strong>14.01.2019 - Feature Descriptors and Interest Points</strong><br />\n",
    "21.01.2019 <strong style=\"color: red\">08.30-10.00</strong> - Image Registration<br />\n",
    "28.01.2019 <strong style=\"color: red\">08.30-10.00</strong> - Solving a Problem<br />\n",
    "\n",
    "The topics are provided here for your orientation and are subject to change, in accordance with the lectures.\n",
    "\n",
    "<h2>Exercise 6: Feature Descriptors and Interest Points</h2>\n",
    "\n",
    "Due date: <strong>21.01.2018</strong>\n",
    "\n",
    "<h3>List of Tasks</h3>\n",
    "<ul>\n",
    "    <li><a href=\"#task1\">Task 1: Finding Corners and Interest Points</a></li>\n",
    "    <li><a href=\"#task2\">Task 2: SIFT, SURF and FAST</a></li>\n",
    "    <li><a href=\"#task3\">Task 3: Blobs</a></li>\n",
    "    <li><a href=\"#task4\">Task 4: Hough Transform</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Local Features, Corners and Interest Points</h3>\n",
    "In computer vision, finding interest point in an image is essential for subsequent processing. An interest point is a point in the image which, in general, has a clear (preferably mathematical) definition, a well-defined position in the image space. Further, the local image structure around the interest point is rich in local information content. Ideally, it is robust under local and global perturbations in the image (illumination/brightness changes), such that it can be computed with a high degree of repeatability.\n",
    "\n",
    "<p>Interest points are used in further processing: image stitching, object recognition, gesture recognition, tracking and many more.</p>\n",
    "\n",
    "<p>You can read the introductory part of the paper <em>Local Invariant Feature Detectors: A Survey</em> by Tuytelaars and Mikolajczyk in PDF format (<code>survey_inv_features.pdf</code>) found in your exercise folder. The paper gives a good introduction to feature descriptors and interest points.</p>\n",
    "\n",
    "<p>During this exercise, you will learn how to use popular interest point detectors in OpenCV.</p>\n",
    "\n",
    "<a name=\"task1\"></a>\n",
    "<h3 style=\"color: red\">Task 1: Finding Corners and Interest Points</h3>\n",
    "\n",
    "Corners and interest points refer to point-like features in the image, which have a local two-dimensional structure. Specifically, a corner can be defined as the intersection of two edges. Most corner detetion algorithms are sensitive to local image regions which have a high gradient change in both x- and y-direction. In practice, most corner detection methods detect interest points in general, and the terms \"corner\" and \"interest point\" are used interchangeably throughout the literature.\n",
    "\n",
    "An interest point detector is an algorithm that chooses points from an image based on a defined criterion. Typically, an interest point is obtained as a local maximum of a function calculated over an image region. A <em>descriptor</em> is a vector of values, which describes the image patch around an interest point. The simplest descriptor would be as simple as the raw pixel values around the interest point, or more complicated, such as a histogram of gradient orientations. The interest point and its descriptor are together called <em>a local feature</em>.\n",
    "\n",
    "<h4>Harris Corner Detector</h4>\n",
    "The Harris corner detector basically finds under a sliding window a change in intensity in all directions. The window function is usually either a rectangular window (1 in window, 0 outside) or a Gaussian. Mathematically, this can be expressed as: \n",
    "\n",
    "$$E(u,v) = \\sum_{x,y} \\underbrace{w(x,y)}_\\text{window function} \\, [\\underbrace{I(x+u,y+v)}_\\text{shifted intensity}-\\underbrace{I(x,y)}_\\text{intensity}]^2$$\n",
    "\n",
    "However, to speed up the calculation, this measure can be approximated by:\n",
    "\n",
    "$$ E(u,v) \\approx \\begin{bmatrix} u & v \\end{bmatrix} M \\begin{bmatrix} u \\\\ v \\end{bmatrix} $$\n",
    "\n",
    "where $M$ is a 2$\\times$2 matrix computed from image derivatives:\n",
    "\n",
    "$$M = \\sum_{x,y} w(x,y) \\begin{bmatrix}I_x I_x & I_x I_y \\\\ I_x I_y & I_y I_y \\end{bmatrix}$$\n",
    "\n",
    "In order to get an understanding what that formula means, consider first a corner with its axes aligned to the image's axes:\n",
    "\n",
    "$$M = \\begin{bmatrix}I_x I_x & I_x I_y \\\\ I_x I_y & I_y I_y \\end{bmatrix} = \\begin{bmatrix}\\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bmatrix}$$\n",
    "\n",
    "If either $\\lambda_1$ or $\\lambda_2$ is close to zero, the patch is not considered to be a corner.\n",
    "\n",
    "In the (general) case of a corner whose axes are not aligned with the image's axes, we can perform an Eigenvalue decomposition on the matrix:\n",
    "\n",
    "$$M = R^{-1}\\begin{bmatrix}\\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bmatrix}R$$\n",
    "\n",
    "and still think the same way.\n",
    "\n",
    "A corner response function:\n",
    "$$ R = \\text{det}(M) - \\alpha \\text{trace}(M)^2$$\n",
    "gives a cornerness score of the patch. $\\alpha$ is empirically chosen to be usually between 0.04 and 0.06.\n",
    "\n",
    "For further understanding, take a look at Rick Szeliski's lecture notes on the Harris corner detector <a href=\"https://courses.cs.washington.edu/courses/cse576/06sp/notes/HarrisDetector.pdf\">which can be found here</a>.\n",
    "\n",
    "<strong style=\"color: red\">Programming Task: Harris Corner Detector</strong>\n",
    "\n",
    "Read about the Harris corner detector in  <a href=\"https://docs.opencv.org/3.4.1/dc/d0d/tutorial_py_features_harris.html\">OpenCV's manuals</a>. Apply the corner detector to the image <code>graffiti_01.png</code> selecting suitable parameters. Visualize the result by drawing small crosses or circles at the corners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Shi-Tomasi Corner Detector</h4>\n",
    "Shi and Tomasi made an improvement to the original corner detector. <a href=\"http://aishack.in/tutorials/shitomasi-corner-detector/\">Read here</a> what the improvement is about.\n",
    "\n",
    "<strong style=\"color: red\">Programming Task: Shi-Tomasi Corner Detector</strong>\n",
    "\n",
    "Read about the Shi-Tomasi corner detector in  <a href=\"https://docs.opencv.org/3.4.1/d8/dd8/tutorial_good_features_to_track.html\">OpenCV's manuals</a>.\n",
    "Apply the corner detector to the image <code>graffiti_01.png</code> and <code>x-ray-hand-1.jpg</code> selecting suitable parameters. Visualize the result by drawing small crosses or circles at the corners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img1)\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: red\">Programming Task: Corners with sub-pixel accuracy</strong>\n",
    "\n",
    "Sometimes, you may need to find the corners with maximum accuracy. OpenCV comes with a function cornerSubPix() which further refines the corners detected with sub-pixel accuracy. The function iterates to find the sub-pixel accurate location of corners or radial saddle points.\n",
    "\n",
    "Extend your implementation of Shi-Tomasi detector with <code>cornerSubPix()</code>. Process both\n",
    "images <code>graffiti_01.png</code> and <code>x-ray-hand-1.jpg</code> with suitable parameters. Use the following line of code as the termination criteria:\n",
    "\n",
    "<pre>\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "</pre>\n",
    "\n",
    "What is the difference to the previous results? Can you explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img1)\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task2\"></a>\n",
    "<h3 style=\"color: red\">Task 2: SIFT, SURF and FAST</h3>\n",
    "\n",
    "All feature detectors observed above are rotation-invariant, which means, even if the image is rotated, we can find the same corners. However, a corner may not be recognized such if the image is scaled. A corner in a small image within a small window is flat when it is zoomed in the same window. Therefore, the Harris corner detector is not scale-invariant.\n",
    "\n",
    "The question, of course, is now how to detect scale-invariant local features? A naïve approach would be to compare descriptors in an exhaustive multi-scale space. However, this approach probably wouldn't really work in practice.\n",
    "\n",
    "<h4>SIFT</h4>\n",
    "\n",
    "In 1999, Lowe came up with a new scale-invariant algorithm, known as SIFT, which extracts key-points and computes their descriptors. SIFT can robustly identify objects even among clutter and under partial occlusion, because the\n",
    "SIFT feature descriptor is invariant to uniform scaling, orientation, illumination changes, and partially invariant to affine distortion. Key locations are defined as maxima and minima of the result of difference of Gaussians function applied in scale space to a series of smoothed and resampled images. Low contrast candidate points and edge\n",
    "response points along an edge are discarded. Dominant orientations are assigned to localized key-points.\n",
    "These steps ensure that the key-points are more stable for matching and recognition.\n",
    "\n",
    "For an in-depth explanation on how the SIFT algorithm works, refer to <a href=\"http://www.aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/\">AI Shack's tutorial SIFT: Theory and Practice</a>.\n",
    "\n",
    "<h4>SURF</h4>\n",
    "SIFT algorithm was rather slow and its speed-up was proposed in 2006 by Bay, Tuytelaars and Van Gool. In SIFT, Lowe approximated Laplacian of Gaussian with Difference of Gaussian for finding scale-space. SURF goes a little further and approximates LoG with Box Filter. One big advantage of this approximation is that, convolution with box filter can be easily calculated with the help of integral images. And it can be done in parallel for different scales.\n",
    "Besides that, SURF relies on the determinant of the Hessian matrix for both scale and location.\n",
    "\n",
    "<h4>FAST</h4>\n",
    "SURF offers a good sped-up to the SIFT algorithm, however, it is still not fast enough for real-time applications. FAST algorithm was proposed by Rosten and Drummond in 2006. The feature detection works as follows:\n",
    "\n",
    "<ol>\n",
    "    <li>Select a pixel $p$ in the image which is to be identified as an interest point or not. Let its intensity\n",
    "be $I_p$.</li>\n",
    "    <li>Select appropriate threshold value $t$.</li>\n",
    "    <li>Consider a circle of 16 pixels around the pixel under test.</li>\n",
    "    <li>Now the pixel $p$ is a corner if there exists a set of $n$ contiguous pixels in the circle (of 16 pixels)\n",
    "which are all brighter than $I_p + t$, or all darker than $I_p − t$. The value of $n$ was chosen to be 12.</li>\n",
    "</ol>\n",
    "\n",
    "    \n",
    "<strong style=\"color: red\">Programming Task: SIFT, SURF and FAST</strong>\n",
    "\n",
    "SIFT used to be included into the official OpenCV distribution before the version 3.0. It was removed in the latest version due to the fact that this algorithm is known to be patented. However, it is still possible to use it in the community version of OpenCV (<a href=\"https://github.com/opencv/opencv_contrib\">https://github.com/opencv/opencv_contrib</a>). This Jupyter Notebook has the community version installed, so you don't need to worry about any installation issues. The same holds for FAST and SURF.\n",
    "\n",
    "Apply the SIFT, SURF and FAST detector to the images <code>graffiti_01.png</code> and <code>x-ray-hand-1.jpg</code>. Visualize the found keypoints.\n",
    "You can find the corresponding tutorials here:\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.opencv.org/3.4.1/da/df5/tutorial_py_sift_intro.html\">SIFT</a></li>\n",
    "    <li><a href=\"https://docs.opencv.org/3.4.1/df/dd2/tutorial_py_surf_intro.html\">SURF</a></li>\n",
    "    <li><a href=\"https://docs.opencv.org/3.4.1/df/d0c/tutorial_py_fast.html\">FAST</a></li>\n",
    "</ul>\n",
    "\n",
    "Compare your results with Fig. 1d and Fig. 2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img1_sift)\n",
    "plt.imshow(img1_surf)\n",
    "plt.imshow(img1_fast)\n",
    "plt.imshow(img2_sift)\n",
    "plt.imshow(img2_surf)\n",
    "plt.imshow(img2_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3\"></a>\n",
    "<h3 style=\"color: red\">Task 3: Blobs</h3>\n",
    "\n",
    "A blob is a group of connected pixels in an image that share some common property. OpenCV\n",
    "offers a convenient way to detect blobs and filter them based on different characteristics. A class\n",
    "<code>SimpleBlobDetector</code> is provided for that purpose. The class implements a simple algorithm for extracting blobs from an image:\n",
    "<ol>\n",
    "    <li>Convert the source image to binary image by applying thresholding with several thresholds from\n",
    "        <code>minThreshold</code> (inclusive) to <code>maxThreshold</code> (exclusive) with distance thresholdStep between neighboring thresholds.</li>\n",
    "    <li>Extract connected components from every binary image by <code>findContours</code> and calculate their cen-\n",
    "ters.</li>\n",
    "    <li>Group centers from several binary images by their coordinates. Close centers form one group that\n",
    "        corresponds to one blob, which is controlled by the <code>minDistBetweenBlobs</code> parameter.</li>\n",
    "    <li>From the groups, estimate final centers of blobs and their radiuses and return as locations and sizes\n",
    "    of keypoints.</li>\n",
    "</ol>\n",
    "\n",
    "<strong style=\"color: red\">Programming Task: Simple Blobs</strong>\n",
    "\n",
    "Apply the simple blob detector on the images <code>mdck0.jpg</code> and <code>mdck1.jpg</code>. Try to get as many (or more) cells detected as in the Fig. 3. You can find introductory tutorials to using OpenCV's simple blob detector with Python <a href=\"https://www.makehardware.com/2016/05/19/blob-detection-with-python-and-opencv/\">here</a> and <a href=\"https://www.learnopencv.com/blob-detection-using-opencv-python-c/\">here</a>.\n",
    "\n",
    "<strong>Hint:</strong> besides parameters adjustment, as explained in the tutorials, some prior preprocessing might needed. It could be a good idea to perform channel splitting at first.\n",
    "\n",
    "Mark the detected blobs and print out the number of detected blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img1)\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4\"></a>\n",
    "<h3 style=\"color: red\">Task 4: Hough Transform</h3>\n",
    "\n",
    "The Hough transform is a feature extraction technique the purpose of which is to find imperfect instances\n",
    "of objects within a certain class of shapes by a voting procedure. This voting procedure is carried\n",
    "out in a parameter space, from which object candidates are obtained as local maxima in a so-called\n",
    "accumulator space that is explicitly constructed by the algorithm for computing the Hough transform.\n",
    "The classical Hough transform was concerned with the identification of lines in the image, but later the\n",
    "Hough transform has been extended to identifying positions of arbitrary shapes, most commonly circles\n",
    "or ellipses.\n",
    "\n",
    "OpenCV offers two versions of Hough line transform: the original one and a probabilistic one. The probabilistic Hough line transform is more efficient implementation of the Hough line transform. It returns as output the extremes of the detected lines $(x_0, y_0, x_1, y_1)$\n",
    "\n",
    "<strong style=\"color: red\">Programming Task: Hough Transform</strong>\n",
    "\n",
    "Apply Hough transform to the <code>color_wound.jpg</code> to detect lines between the color patches in the color card. Make sure to binarize your initial image using Canny edge detection with appropriate\n",
    "parameters. OpenCV's tutorial on the Hough line transform <a href=\"https://docs.opencv.org/3.4.1/d9/db0/tutorial_hough_lines.html\">can be found here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: red\">Programming Task: Probabilistic Hough Transform</strong>\n",
    "\n",
    "Apply probabilistic Hough transform to the <code>color_wound.jpg</code> chosing appropriate parameters which will allow a better detection of the lines in the card. Try to minimize the number of outliers (lines detected outside the card)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hough circle transform works in a roughly analogous way to the Hough line transform.\n",
    "In the line detection case, a line was defined by two parameters $(\\rho, \\theta)$. In the circle case, we need three\n",
    "parameters to define a circle: $C: (x_{center}, y_{center},\\rho)$, where $(x_{center}, y_{center})$  define the center position and $\\rho$ is the radius, which allows us to completely define a circle. For sake of efficiency, OpenCV implements a detection method slightly trickier than the standard Hough transform: The Hough gradient method.\n",
    "\n",
    "<strong style=\"color: red\">Programming Task: Hough Transform for Circles</strong>\n",
    "\n",
    "Apply circle detection to the images <code>DH4_AF_CCD_B.jpg</code> and <code>DH4_SF_ICD_B.jpg</code>. Select the parameters in such way so that the same set of parameters would work for both of the images. A hint: apply median blur prior to Hough transform. Adjust the parameters to detect the pupil in the image <code>pupil_detection.bmp</code>. The results shall be similar to Figure 5. Now try to detect the iris.\n",
    "OpenCV's tutorial on the Hough circle transform <a href=\"https://docs.opencv.org/3.4.1/d4/d70/tutorial_hough_circle.html\">can be found here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: red\">Bonus Programming Task: Own Implementation of the Hough Transform for Circles</strong>\n",
    "\n",
    "As a bonus task, now that you are knowledgeable about the Hough transform, write your own implementation of the Circle Hough transform and find circles in the two images <code>DH4_AF_CCD_B.jpg</code> and <code>DH4_SF_ICD_B.jpg</code>. Remember that a circle can be represented by the following equation:\n",
    "\n",
    "<strong>Hint:</strong> A circle can be parameterized by the following equation:$$ ( x − a )^2 + ( y − b )^2 = r^2 $$\n",
    "This would give us three parameters: $a$, $b$, and $r$ and therefore a 3-dimensional parameter space. Both  computation  and  memory  requirements increase exponentially with the number of parameters. \n",
    "\n",
    "Without gradient information, all values $(a,b)$ lying on the cone given by the circle equation are incremented. With\n",
    "the gradient information, we only need to increment points on an arc centered at $(a,b)$:\n",
    "\n",
    "$$a =  x  + r \\cos\\phi, b=y+r\\sin\\phi$$\n",
    "where $\\phi$ is the gradient angle returned by the edge operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "# YOUR CODE HERE \n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"features-graffiti.png\" />\n",
    "<strong>Figure 1.</strong> Features detected in the image <code>graffiti_01.png</code>: (a) Harris corner detector; (b) Shi-Tomasi; (c) Corners with subpixel accuracy; (d) FAST.\n",
    "\n",
    "<img src=\"features-hand.png\" />\n",
    "<strong>Figure 2.</strong> Features detected in the image <code>x−ray−hand−1.jpg</code>: (a) Harris corner detector; (b) Shi-Tomasi; (c) Corners with subpixel accuracy; (d) FAST.\n",
    "\n",
    "<img src=\"blobs.png\" />\n",
    "<strong>Figure 3.</strong> Detected blobs: (a) 13 blobs detected; (b) 16 blobs detected.\n",
    "\n",
    "<img src=\"hough-lines.png\" />\n",
    "<strong>Figure 4.</strong> Line detection using Hough transform: (a) original; (b) probabilistic.\n",
    "\n",
    "<img src=\"hough-circles.png\" />\n",
    "<strong>Figure 5.</strong> Circle detection using Hough transform: (a) tooth 1; (b) tooth 2; (c) eye pupil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
