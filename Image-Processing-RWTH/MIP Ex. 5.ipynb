{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Image Processing and Handling WS 2018/19</h1>\n",
    "\n",
    "Exercise instructor: Marko Jovanović, mjovanovic@mi.rwth-aachen.de\n",
    "\n",
    "<strong style=\"color: red\">Notice: </strong>Attendance to <strong>all</strong> exercises sessions <strong>is mandatory</strong>. However, submitted exercise solutions aren't graded nor they present a prerequiste for the exam, but you will receive feedback on your submitted solutions.\n",
    "\n",
    "The exercise sessions are held from 12.30-14.00 on the following dates at COMA 1:\n",
    "\n",
    "22.10.2018 - OpenCV and Python intro (this session)<br />\n",
    "05.11.2018 - Image Enhancement<br />\n",
    "19.11.2018 - Fourier Transform<br />\n",
    "03.12.2018 - Low-level Image Segmentation<br />\n",
    "<strong>17.12.2018 - High-level Image Segmentation</strong><br />\n",
    "14.01.2019 - Visualization<br />\n",
    "21.01.2019 - Automation<br />\n",
    "28.01.2019 - Solving a Problem<br />\n",
    "\n",
    "The topics are an orientation and subject to change, in accordance with the lectures.\n",
    "\n",
    "<h2>Exercise 4: High-level Image Segmentation</h2>\n",
    "\n",
    "Due date: <strong>24.12.2018</strong>\n",
    "\n",
    "<h3>Semantic Segmentation</h3>\n",
    "\n",
    "Semantic Segmentation has the aim to assign object classes to each image pixel. Unlike simple object detection, where we try to find and locate objects, possibly with their bounding boxes, we aim to delineate objects with dense, pixel-wise prodeictions from our models.\n",
    "\n",
    "In the lecture, you have been taught several segmentation methods, including several pixel-based methods such as Otsu; edge-based methods such as active countours and watershed method; region growing; model-based such as the Hough-transform. In the last exercise, you had to deal with programming a thresholding method, clustering K-means, Mean-shift, Canny-edge detector, Sobel edge-detector and a graph-cut segmentation method.\n",
    "\n",
    "Also, during the lecture, you have been introduced to artificial neural networks (ANN).\n",
    "\n",
    "During this exercise, you will learn about another segmentation method based on deep-learning. Deep learning is part of a broader family of machine learning algorithms which are  based on learning data representations (as opposed to task-specific algorithms). A Deep neural network (DNN) is a fancy term for an artificial neural network (ANN) with multiple layers between the input and output layers. You will learn how to apply semantic segmentation to extract a dense, pixel-wise map of each of these classes in both images and video streams using such a DNN.\n",
    "\n",
    "The semantic segmentation architecture we’re using for this exercise is ENet, which is based on Paszke et al.’s 2016 publication, <a href=\"https://arxiv.org/abs/1606.02147\" target=\"_blank\">ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation.</a>\n",
    "\n",
    "Here is a diagram of the network's architecture:\n",
    "\n",
    "<img src=\"architecture.jpg\" />\n",
    "\n",
    "One of the primary benefits of ENet is that it’s fast — up to 18x faster and requiring 79x fewer parameters with similar or better accuracy than larger models. The model size itself is only 3.2MB! This makes it perfectly suitable for real-time applications and object tracking in video sequences.\n",
    "\n",
    "Paszke et al. trained that dataset on <a href=\"https://www.cityscapes-dataset.com/dataset-overview/\" target=\"_blank\">The Cityscapes Dataset</a>, a semantic, instance-wise, dense pixel annotation of 20-30 classes (depending on which model you’re using). The model we will be using distinguishes the following classes:\n",
    "\n",
    "<ul>\n",
    "    <li>Unlabeled (i.e., background)</li>\n",
    "    <li>Road</li>\n",
    "    <li>Sidewalk</li>\n",
    "    <li>Building</li>\n",
    "    <li>Wall</li>\n",
    "    <li>Fence</li>\n",
    "    <li>Pole</li>\n",
    "    <li>TrafficLight</li>\n",
    "    <li>TrafficSign</li>\n",
    "    <li>Vegetation</li>\n",
    "    <li>Terrain</li>\n",
    "    <li>Sky</li>\n",
    "    <li>Person</li>\n",
    "    <li>Rider</li>\n",
    "    <li>Car</li>\n",
    "    <li>Truck</li>\n",
    "    <li>Bus</li>\n",
    "    <li>Train</li>\n",
    "    <li>Motorcycle</li>\n",
    "    <li>Bicycle</li>\n",
    "</ul>\n",
    "\n",
    "Note that this exercise is purely demonstrational. You don't need to write code this time, rather just learn how to use the provided architecture in action in order to perform the semantical segmentation task.\n",
    "\n",
    "Please note that this exercise's example code uses a new library we haven't introduced before: https://github.com/jrosebr1/imutils \n",
    "\n",
    "Get acquainted with imutils well, as it provides a few convenience functions which you may also use during the exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# path to deep learning segmentation model\n",
    "model = 'opencv-semantic-segmentation/enet-cityscapes/enet-model.net' \n",
    "\n",
    "# path to .txt file containing class labels\n",
    "classes = 'opencv-semantic-segmentation/enet-cityscapes/enet-classes.txt'\n",
    "\n",
    "# path to input image\n",
    "image = 'opencv-semantic-segmentation/images/example_01.png'\n",
    "\n",
    "# path to .txt file containing colors for labels\n",
    "colors = 'opencv-semantic-segmentation/enet-cityscapes/enet-colors.txt'\n",
    "\n",
    "# desired width (in pixels) of input image\n",
    "width = 500 \n",
    "\n",
    "# load the class label names\n",
    "CLASSES = open(classes).read().strip().split(\"\\n\")\n",
    " \n",
    "# if a colors file was supplied, load it from disk\n",
    "if colors:\n",
    "\tCOLORS = open(colors).read().strip().split(\"\\n\")\n",
    "\tCOLORS = [np.array(c.split(\",\")).astype(\"int\") for c in COLORS]\n",
    "\tCOLORS = np.array(COLORS, dtype=\"uint8\")\n",
    "    # otherwise, we need to randomly generate RGB colors for each class label\n",
    "else:\n",
    "\t# initialize a list of colors to represent each class label in\n",
    "\t# the mask (starting with 'black' for the background/unlabeled\n",
    "\t# regions)\n",
    "\tnp.random.seed(42)\n",
    "\tCOLORS = np.random.randint(0, 255, size=(len(CLASSES) - 1, 3), dtype=\"uint8\")\n",
    "\tCOLORS = np.vstack([[0, 0, 0], COLORS]).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the legend visualization\n",
    "legend = np.zeros(((len(CLASSES) * 25) + 25, 300, 3), dtype=\"uint8\")\n",
    " \n",
    "# loop over the class names + colors\n",
    "for (i, (className, color)) in enumerate(zip(CLASSES, COLORS)):\n",
    "\t# draw the class name + color on the legend\n",
    "\tcolor = [int(c) for c in color]\n",
    "\tcv2.putText(legend, className, (5, (i * 25) + 17),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\tcv2.rectangle(legend, (100, (i * 25)), (300, (i * 25) + 25),\n",
    "\t\ttuple(color), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are using the DNN module from OpenCV.\n",
    "See https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV, and https://www.pyimagesearch.com/2017/08/21/deep-learning-with-opencv/ for an overview of the module's capabilities.\n",
    "\n",
    "Please, take a moment and have a look at what cv2.dnn.blobFromImage does:\n",
    "\n",
    "https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] inference took 0.2590 seconds\n"
     ]
    }
   ],
   "source": [
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNet(model)\n",
    " \n",
    "# load the input image, resize it, and construct a blob from it,\n",
    "# but keeping mind mind that the original input image dimensions\n",
    "# ENet was trained on 1024x512\n",
    "image = cv2.imread(image)\n",
    "image = imutils.resize(image, width=width)\n",
    "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (1024, 512), 0, swapRB=True, crop=False)\n",
    " \n",
    "# perform a forward pass using the segmentation model\n",
    "net.setInput(blob)\n",
    "start = time.time()\n",
    "output = net.forward()\n",
    "end = time.time()\n",
    " \n",
    "# show the amount of time inference took\n",
    "print(\"[INFO] inference took {:.4f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer the total number of classes along with the spatial dimensions\n",
    "# of the mask image via the shape of the output array\n",
    "(numClasses, height, width) = output.shape[1:4]\n",
    " \n",
    "# our output class ID map will be num_classes x height x width in\n",
    "# size, so we take the argmax to find the class label with the\n",
    "# largest probability for each and every (x, y)-coordinate in the\n",
    "# image\n",
    "classMap = np.argmax(output[0], axis=0)\n",
    " \n",
    "# given the class ID map, we can map each of the class IDs to its\n",
    "# corresponding color\n",
    "mask = COLORS[classMap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b4c812710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.text.Text at 0x7f4acc162860>,\n",
       " ([], <a list of 0 Text xticklabel objects>),\n",
       " ([], <a list of 0 Text yticklabel objects>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize the mask and class map such that its dimensions match the\n",
    "# original size of the input image (we're not using the class map\n",
    "# here for anything else but this is how you would resize it just in\n",
    "# case you wanted to extract specific pixels/classes)\n",
    "mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "classMap = cv2.resize(classMap, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    " \n",
    "# perform a weighted combination of the input image with the mask to\n",
    "# form an output visualization\n",
    "output = ((0.4 * image) + (0.6 * mask)).astype(\"uint8\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "\n",
    "plt.subplot(121), plt.imshow(image)\n",
    "plt.title('Input image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122), plt.imshow(legend)\n",
    "plt.title('Legend'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(111), plt.imshow(output)\n",
    "plt.title('Output image'), plt.xticks([]), plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, performing semantic image segmentation with a pre-trained model is a fairly easy task.\n",
    "\n",
    "Read this tutorial if you're interested on training your own ENet model: https://github.com/TimoSaemann/ENet/tree/master/Tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
